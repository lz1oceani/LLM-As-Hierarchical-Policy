# Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving 

This repo contains the code, prompts, and model outputs for [Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving](https://arxiv.org/pdf/2311.00694.pdf).

Current methods for enabling Large Language Models (LLMs) to solve challenging reasoning problems often fall short due to limited exploration capabilities. In this work, we enhance LLMs' problem-solving strategies by framing them as a **hierarchical policy** through in-context learning. This consists of a visionary **leader** proposing diverse high-level tactics and a **follower** executing detailed processes per each tactic. In addition, we introduce a tournament-based approach to efficiently select the best solution from the generated solution groups. As a result, Our approach fosters strategic exploration, generates insightful hints, and improves answer accuracy on challenging problems.

Here are teasers for our methods:

![method](https://github.com/lz1oceani/LLM-As-Hierarchical-Policy-Test/blob/master/images/teaser.jpg)
![example](https://github.com/lz1oceani/LLM-As-Hierarchical-Policy-Test/blob/master/images/example.jpg)

## Setup
You need to first have an OpenAI API key and store it as the environment variable ``OPENAI_API_KEY`` (see [here](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)).

To install the necessary dependencies, run the following command:
``pip install -e .``

## Experiments
You can find prompts for instructing LLMs to generate and compare reasoning chains in the [prompts](hlm/prompts/) directory. These include:
- [0shot CoT](hlm/prompts/0shot.txt)
- [follow the demonstration example](hlm/prompts/dynamic_1shot.txt)
- [use the given tactic](hlm/prompts/use_techniques.txt)
- [compare two reasoning chains](hlm/prompts/compare_answer.txt)

> 1. To generate reasoning chains with 0shot prompt or high-level tactics, use the following command:

``python hlm/app/evaluate_qa.py --dataset MATH --dataset-args=DATASET_ARGS --dataset-name=DATASET_NAME --reasoning-args=REASONING_ARGS``

Explanation of key arguments:
- `DATASET_ARGS`: A dictionary specifying the dataset arguments, e.g., `"dict(levels=5, subsample=20, include_asy=False)"`.
- `DATASET_NAME`: The name of the dataset, which may differ from the value passed to `--dataset`.
- `REASONING_ARGS`: Specifies the reasoning strategies, e.g., 0-shot or generated tactics. For more details, refer to the [scripts](scripts/) directory.

> 2. To compute Grouped-Majority Recall.
Please see `hlm/app/computer_gmr.py` for details.

> 3. Run tournament over group majorities.
In some questions, all group majorities may be the same, or there might be no correct answer among them. For these cases, we do not need to run tournament. To reduce the cost, you can use `hlm/app/get_examples_for_tournament.py` to generate the filtered questions before running a tournament. Then, execute `hlm/app/run_tournament.py` to obtain the final accuracy.

> 4. Results of more question from level 5 of MATH dataset can be downloaded [here](https://drive.google.com/file/d/1ZWCoIwcsc3tSdfpRfhw1Bj2kFr2nQ37N/view?usp=sharing).

## File Structure and Data Format

### Reasoning Chains
``results/math_hard_20/reasoning_output/gpt-3.5-turbo/{exploration_mode}.json`` contains reasoning chains generated by ChatGPT (GPT-3.5-turbo). Each problem may have up to 64 candidate reasoning chains. The file format is as follows:

```javascript
{
  "example_idx", // example idx from the original dataset
  "final_answer", // ground truth solution of this question
  "reasoning_inputs", // the prompts for generating reasoning chains
  "prompt_infos", // the detailed information of the prompts
  "reasoning_raw_outputs", // the the candidate reasoning chains generated by LLM grouped by different prompts
  "reasoning_outputs", // the reasoning chains generated by LLM
  "pred_answers", // the final answers extracted from the the candidate reasoning chains
  "groups", // groups of the final answers extracted from the the candidate reasoning chains
  "per_sample_correct", // whether each final answer is correct or not
  "majority_results", // the final answer(s) based on majority voting over the candidates; note that there can be multiple results after majority voting if they receive the same number of votes
  "majority_corrects", // whether each of the majority_results is correct
}
```

### Tactics generated by GPT-4
```results/gpt4_answer_techniques.json``` contains a collection of tactics and techniques generated by GPT-4 on level 5 of MATH dataset.
```javascript
{
  "example_idx", // example idx from the original dataset
  "question", // question
  "answer_techniques_prompt", // the prompts for generated tactics
  "answer_techniques", // generated tactics
}
```

### Tournament Results
``results/math_hard_20/tournament/{llm_name}-{exploration_mode}.json`` files store tournament results. For each problem, the best group is iteratively compared against incoming groups. The file format is as follows:

```javascript
[
    [
        {
        "solution1", // solutions from the first group
        "solution2", // solutions from the second group
        "prompts", // prompts for compare sampled solutions from group1 and group2
        "outputs", // comparison results generated by LLM
        "processed_outputs",  // parsed results from "outputs"
        "voting", // the voting results of this compassion
        }
    ],
    [best_answer], // best response selected by the tournament
]
```

## Citation
Please cite our paper if you find our idea helpful. Thanks a lot!

```
@article{ling2023unleashing,
  title={Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving},
  author={Ling, Zhan and Fang, Yunhao and Li, Xuanlin and Mu, Tongzhou and Lee, Mingu and Pourreza, Reza and Memisevic, Roland and Su, Hao},
  journal={arXiv preprint arXiv:2311.00694},
  year={2023}
}
```

## License

This project is licensed under the CC-BY-4.0 License.
